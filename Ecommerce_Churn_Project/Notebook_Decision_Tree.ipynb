{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharWarr/ML_Projects/blob/main/Ecommerce_Churn_Project/Notebook_Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9beO804q15O"
      },
      "outputs": [],
      "source": [
        "# Setting the environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bhA2P45q15P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook --no-browser\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
        "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3vzidc3q15Q"
      },
      "outputs": [],
      "source": [
        "# Spark environment\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-Rzyjk5q15Q",
        "outputId": "2bcdc4ac-e60b-4f29-aab7-7e9dc88785ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/02/20 13:25:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ip-172-31-92-239.ec2.internal:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>demo</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe516e67d90>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAX_MEMORY = \"14G\"\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"demo\") \\\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTF0UmrUq15S"
      },
      "source": [
        "### Data description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtq8JKXUq15S"
      },
      "source": [
        "The dataset stores the information of a customer session on the e-commerce platform. It records the activity and the associated parameters with it.\n",
        "\n",
        "- **event_time**: Date and time when user accesses the platform\n",
        "- **event_type**: Action performed by the customer\n",
        "            - View\n",
        "            - Cart\n",
        "            - Purchase\n",
        "            - Remove from cart\n",
        "- **product_id**: Unique number to identify the product in the event\n",
        "- **category_id**: Unique number to identify the category of the product\n",
        "- **category_code**: Stores primary and secondary categories of the product\n",
        "- **brand**: Brand associated with the product\n",
        "- **price**: Price of the product\n",
        "- **user_id**: Unique ID for a customer\n",
        "- **user_session**: Session ID for a user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWyo9RGNq15S"
      },
      "source": [
        "### Initialising the SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNqFQY2cq15S"
      },
      "source": [
        "The dataset provided is 5 GBs in size. Therefore, it is expected that you increase the driver memory to a greater number. You can refer to notebook 1 for the steps involved here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWuV4YW4q15T",
        "outputId": "824b8d51-6a16-4fec-fa02-f0efa5248b02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Loading the clean data\n",
        "df=spark.read.parquet(\"Cleaned_df_final_parquet.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvVRxufVq15T"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "bucketizer = Bucketizer(splits=[ 0, 6, 12, 18, 24 ],inputCol=\"Hour\", outputCol=\"Hour_binned\")\n",
        "df_buck = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
        "\n",
        "from pyspark.sql.types import IntegerType,FloatType\n",
        "df_buck = df_buck.withColumn(\"Hour_binned\", df_buck[\"Hour_binned\"].cast(IntegerType()))\n",
        "\n",
        "# Check if only the required columns are present to build the model\n",
        "# If not, drop the redundant columns\n",
        "df_buck = df_buck.fillna(value ='no category',subset =['category_2'])\n",
        "df_buck = df_buck.withColumn(\"price\", df_buck[\"price\"].cast(FloatType()))\n",
        "df_buck = df_buck.drop(\"category_code\",\"user_id\",\"product_id\",\"brand\",\"Hour\",\"category_id\",\"user_session\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mVtd4nVq15T"
      },
      "outputs": [],
      "source": [
        "# cast the click column to interger data type. \n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_buck = df_buck.withColumn(\"y\", df_buck[\"target\"].cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5wXft57q15T"
      },
      "outputs": [],
      "source": [
        "from math import floor\n",
        "from pyspark.sql.functions import rand\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def stratifiedSample(df, N, labelCol=\"y\"):\n",
        "    ctx = df.groupby(labelCol).count()\n",
        "    ctx = ctx.withColumn('frac', col(\"count\") / df.count())\n",
        "    frac = ctx.select(\"y\", \"frac\").rdd.collectAsMap()\n",
        "    pos = int(floor(frac[1] * N))\n",
        "    neg = int(floor(frac[0] * N))\n",
        "    posDF = df.filter(col(labelCol) == 1).orderBy(rand()).limit(pos)\n",
        "    negDF = df.filter(col(labelCol) == 0).orderBy(rand()).limit(neg)\n",
        "    return posDF.unionAll(negDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fymlApGcq15T",
        "outputId": "64669f62-761a-44b5-d31d-ead8f1ef0ae2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "xdf = stratifiedSample(df_buck, 5_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTb-cncQq15U",
        "outputId": "16ae9d5f-40fe-4dab-b42f-ce7bc05f6af9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------+-----------+--------------------+---------+------+-----------+---+------------------+\n",
            "|event_type| price| category_1|          category_2|brand_new|target|Hour_binned|  y|Hour_binned_posEnc|\n",
            "+----------+------+-----------+--------------------+---------+------+-----------+---+------------------+\n",
            "|      view|341.74|electronics|         no category|   xiaomi|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view| 36.04|no category|         no category| no brand|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view| 34.11|no category|         no category|   Others|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view| 63.06|no category|         no category|   Others|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|341.91|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|362.34|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|341.91|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|392.38|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|339.28|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|448.84|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|283.12| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|225.23| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|283.12| appliances|kitchen.refrigera...|   Others|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|225.23| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|228.47| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|283.12| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|952.03|electronics|         no category|   huawei|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|196.91|electronics|         no category|   xiaomi|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view|153.98|electronics|         no category|   huawei|     0|          0|  0|     (3,[0],[1.0])|\n",
            "|      view|166.54|electronics|         no category|   huawei|     0|          0|  0|     (3,[0],[1.0])|\n",
            "+----------+------+-----------+--------------------+---------+------+-----------+---+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# onehot encoding \n",
        "#import the onehot encoder\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "#create the encoder object\n",
        "ohe = OneHotEncoderEstimator(inputCols=['Hour_binned'], outputCols=['Hour_binned_posEnc'])\n",
        "#fit the obejct to the dataframe\n",
        "oh_encoder = ohe.fit(df_buck)\n",
        "#tranform the dataframe, by adding the \n",
        "encoded = oh_encoder.transform(df_buck)\n",
        "\n",
        "encoded.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8D1ONzVq15U",
        "outputId": "85be2b5f-3e13-4312-ef35-17c1dcc9b626"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|event_type_ixEnc|\n",
            "+----------------+\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import the string indexer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "#similar to the one hot encoder, create a string indexer object and fit it to the dataframe, use the fitted object to transform the dataset.\n",
        "si = StringIndexer(inputCol='event_type', outputCol='event_type_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['event_type_ix'], outputCols=['event_type_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"event_type_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSh23RYjq15U",
        "outputId": "3b67052b-3395-4e77-ee8b-84967c5d700c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|category_1_ixEnc|\n",
            "+----------------+\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#similar to the one hot encoder, create a string indexer object and fit it to the dataframe, use the fitted object to transform the dataset.\n",
        "si = StringIndexer(inputCol='category_1', outputCol='category_1_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['category_1_ix'], outputCols=['category_1_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"category_1_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "8qzGmCEeq15U",
        "outputId": "b5756772-f0ff-4e14-9064-0ab384bd56e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|category_2_ixEnc|\n",
            "+----------------+\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "si = StringIndexer(inputCol='category_2', outputCol='category_2_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['category_2_ix'], outputCols=['category_2_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"category_2_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsV1zsS_q15U",
        "outputId": "46660273-425f-4db2-d0a7-312150ae2f20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|brand_new_ixEnc|\n",
            "+---------------+\n",
            "| (20,[4],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[0],[1.0])|\n",
            "| (20,[0],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[0],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[5],[1.0])|\n",
            "| (20,[4],[1.0])|\n",
            "| (20,[5],[1.0])|\n",
            "| (20,[5],[1.0])|\n",
            "+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "si = StringIndexer(inputCol='brand_new', outputCol='brand_new_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['brand_new_ix'], outputCols=['brand_new_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"brand_new_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ-wuAJfq15U",
        "outputId": "f4c66111-bf90-4eda-c760-d56c424bdda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|(123,[1,3,5,18,10...|\n",
            "|(123,[1,3,6,18,10...|\n",
            "|(123,[1,3,6,18,10...|\n",
            "|(123,[2,3,6,18,10...|\n",
            "|(123,[2,3,6,18,10...|\n",
            "|(123,[2,3,6,18,10...|\n",
            "|(123,[2,3,6,18,10...|\n",
            "|(123,[2,3,6,18,10...|\n",
            "|(123,[2,3,6,18,10...|\n",
            "|(123,[2,3,6,18,10...|\n",
            "|(123,[2,3,7,21,10...|\n",
            "|(123,[2,3,7,21,10...|\n",
            "|(123,[2,3,7,21,10...|\n",
            "|(123,[2,3,7,21,10...|\n",
            "|(123,[2,3,7,21,10...|\n",
            "|(123,[2,3,7,21,10...|\n",
            "|(123,[2,3,5,18,10...|\n",
            "|(123,[1,3,5,18,10...|\n",
            "|(123,[0,3,5,18,10...|\n",
            "|(123,[0,3,5,18,10...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#import the vector assembler \n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#create the assembler object\n",
        "assembler = VectorAssembler(inputCols=['Hour_binned_posEnc',\n",
        " 'event_type_ixEnc',\n",
        " 'category_1_ixEnc',\n",
        " 'category_2_ixEnc',\n",
        " 'brand_new_ixEnc'], outputCol=\"features\")\n",
        "#transform the data frame using the assembler object. \n",
        "encoded = assembler.transform(encoded)\n",
        "encoded.select(\"features\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P110bNCvq15V"
      },
      "outputs": [],
      "source": [
        "model_df_encoded = encoded.select(\"features\",\"target\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcO_JpJ6q15V"
      },
      "outputs": [],
      "source": [
        "training_df , test_df = model_df_encoded.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHGOw53gq15V",
        "outputId": "1f1afb9b-5f69-413b-d9b3-3e4f8b68bba7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "29692484"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCz6OKaLq15V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTx6lwjQq15V"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWroqWNNq15V"
      },
      "source": [
        "## Task 3: Model Selection\n",
        "3 models for classification:\t\n",
        "- Logistic Regression\n",
        "- Decision Tree\n",
        "- Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjD0HQ7Rq15V"
      },
      "source": [
        "### Model 2: Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5vn9RrLq15W"
      },
      "outputs": [],
      "source": [
        "# Additional steps for Decision Trees, if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GjfyDV4q15W"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ViPs1rWtq15W",
        "outputId": "96f94f84-f7e3-446e-d137-0dafea4a6c3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['event_type',\n",
              " 'price',\n",
              " 'category_1',\n",
              " 'category_2',\n",
              " 'brand_new',\n",
              " 'target',\n",
              " 'Hour_binned']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_buck.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwZYjDc8q15W"
      },
      "source": [
        "#### Feature Transformation (Code will be same; check for the columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIraxuM_q15W",
        "outputId": "37e91cae-47b4-46f1-c5c3-be68f0960bb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "# Feature transformation for categorical features\n",
        "indexer = StringIndexer(inputCol=\"event_type\", outputCol=\"event_type_cat\")\n",
        "indexed = indexer.fit(df_buck).transform(df_buck)\n",
        "# Feature transformation for categorical features\n",
        "indexer = StringIndexer(inputCol=\"category_1\", outputCol=\"category_1_cat\")\n",
        "indexed = indexer.fit(indexed).transform(indexed)\n",
        "# Feature transformation for categorical features\n",
        "indexer = StringIndexer(inputCol=\"category_2\", outputCol=\"category_2_cat\")\n",
        "indexed = indexer.fit(indexed).transform(indexed)\n",
        "# Feature transformation for categorical features\n",
        "indexer = StringIndexer(inputCol=\"brand_new\", outputCol=\"brand_new_cat\")\n",
        "indexed = indexer.fit(indexed).transform(indexed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "oe2isTLNq15W",
        "outputId": "412cff38-53d2-4f07-afe1-5fe8f8d9d302"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['event_type',\n",
              " 'price',\n",
              " 'category_1',\n",
              " 'category_2',\n",
              " 'brand_new',\n",
              " 'target',\n",
              " 'Hour_binned',\n",
              " 'event_type_cat',\n",
              " 'category_1_cat',\n",
              " 'category_2_cat',\n",
              " 'brand_new_cat']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indexed.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVNfTY_Lq15W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP2nLmF6q15W"
      },
      "outputs": [],
      "source": [
        "#Creating Vector Assembler to combine all the raw features\n",
        "# Vector assembler to combine all the features\n",
        "assembler = VectorAssembler(inputCols=[\n",
        " 'price',\n",
        " 'Hour_binned',\n",
        " 'event_type_cat',\n",
        " 'category_1_cat',\n",
        " 'brand_new_cat'], outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfPzfhx5q15W"
      },
      "outputs": [],
      "source": [
        "output = assembler.transform(indexed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEWiTNUiq15X",
        "outputId": "cb28bd12-0276-43a7-fd43-c5739f5d32a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------+-----------+--------------------+---------+------+-----------+--------------+--------------+--------------+-------------+--------------------+\n",
            "|event_type| price| category_1|          category_2|brand_new|target|Hour_binned|event_type_cat|category_1_cat|category_2_cat|brand_new_cat|            features|\n",
            "+----------+------+-----------+--------------------+---------+------+-----------+--------------+--------------+--------------+-------------+--------------------+\n",
            "|      view|341.74|electronics|         no category|   xiaomi|     0|          1|           0.0|           0.0|           0.0|          4.0|[341.739990234375...|\n",
            "|      view| 36.04|no category|         no category| no brand|     0|          1|           0.0|           1.0|           0.0|          1.0|[36.0400009155273...|\n",
            "|      view| 34.11|no category|         no category|   Others|     0|          1|           0.0|           1.0|           0.0|          0.0|[34.1100006103515...|\n",
            "|      view| 63.06|no category|         no category|   Others|     0|          2|           0.0|           1.0|           0.0|          0.0|[63.0600013732910...|\n",
            "|      view|341.91|no category|         no category| no brand|     0|          2|           0.0|           1.0|           0.0|          1.0|[341.910003662109...|\n",
            "|      view|362.34|no category|         no category| no brand|     0|          2|           0.0|           1.0|           0.0|          1.0|[362.339996337890...|\n",
            "|      view|341.91|no category|         no category| no brand|     0|          2|           0.0|           1.0|           0.0|          1.0|[341.910003662109...|\n",
            "|      view|392.38|no category|         no category| no brand|     0|          2|           0.0|           1.0|           0.0|          1.0|[392.380004882812...|\n",
            "|      view|339.28|no category|         no category| no brand|     0|          2|           0.0|           1.0|           0.0|          1.0|[339.279998779296...|\n",
            "|      view|448.84|no category|         no category| no brand|     0|          2|           0.0|           1.0|           0.0|          1.0|[448.839996337890...|\n",
            "|      view|283.12| appliances|kitchen.refrigera...| no brand|     0|          2|           0.0|           2.0|           3.0|          1.0|[283.119995117187...|\n",
            "|      view|225.23| appliances|kitchen.refrigera...| no brand|     0|          2|           0.0|           2.0|           3.0|          1.0|[225.229995727539...|\n",
            "|      view|283.12| appliances|kitchen.refrigera...|   Others|     0|          2|           0.0|           2.0|           3.0|          0.0|[283.119995117187...|\n",
            "|      view|225.23| appliances|kitchen.refrigera...| no brand|     0|          2|           0.0|           2.0|           3.0|          1.0|[225.229995727539...|\n",
            "|      view|228.47| appliances|kitchen.refrigera...| no brand|     0|          2|           0.0|           2.0|           3.0|          1.0|[228.470001220703...|\n",
            "|      view|283.12| appliances|kitchen.refrigera...| no brand|     0|          2|           0.0|           2.0|           3.0|          1.0|[283.119995117187...|\n",
            "|      view|952.03|electronics|         no category|   huawei|     0|          2|           0.0|           0.0|           0.0|          5.0|[952.030029296875...|\n",
            "|      view|196.91|electronics|         no category|   xiaomi|     0|          1|           0.0|           0.0|           0.0|          4.0|[196.910003662109...|\n",
            "|      view|153.98|electronics|         no category|   huawei|     0|          0|           0.0|           0.0|           0.0|          5.0|(5,[0,4],[153.979...|\n",
            "|      view|166.54|electronics|         no category|   huawei|     0|          0|           0.0|           0.0|           0.0|          5.0|(5,[0,4],[166.539...|\n",
            "+----------+------+-----------+--------------------+---------+------+-----------+--------------+--------------+--------------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "n9baUn8-q15X",
        "outputId": "e8915e6b-05dd-4c1b-e61b-e73529c9ea48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|            features|target|\n",
            "+--------------------+------+\n",
            "|[341.739990234375...|     0|\n",
            "|[36.0400009155273...|     0|\n",
            "|[34.1100006103515...|     0|\n",
            "|[63.0600013732910...|     0|\n",
            "|[341.910003662109...|     0|\n",
            "|[362.339996337890...|     0|\n",
            "|[341.910003662109...|     0|\n",
            "|[392.380004882812...|     0|\n",
            "|[339.279998779296...|     0|\n",
            "|[448.839996337890...|     0|\n",
            "|[283.119995117187...|     0|\n",
            "|[225.229995727539...|     0|\n",
            "|[283.119995117187...|     0|\n",
            "|[225.229995727539...|     0|\n",
            "|[228.470001220703...|     0|\n",
            "|[283.119995117187...|     0|\n",
            "|[952.030029296875...|     0|\n",
            "|[196.910003662109...|     0|\n",
            "|(5,[0,4],[153.979...|     0|\n",
            "|(5,[0,4],[166.539...|     0|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if only the required columns are present to build the model\n",
        "# If not, drop the redundant columns\n",
        "output.select(\"features\",\"target\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvNIuIKzq15X"
      },
      "outputs": [],
      "source": [
        "#model_df = output.select(\"features\",\"target\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M-PMUmeq15X"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into train and test (Remember you are expected to compare the model later)\n",
        "#training_df, test_df = model_df.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBUXLHviq15X",
        "outputId": "0ca2a7f9-275c-481c-8bf8-2a02e9b2f001"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "29693021"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of rows in train and test data\n",
        "training_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvlCXyVAq15X",
        "outputId": "6dca5ecd-3136-48ae-bd8b-21fe779ec121"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "12725523"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBRGXlWnq15X"
      },
      "outputs": [],
      "source": [
        "# Pipeline for the tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-vpysk5q15X"
      },
      "outputs": [],
      "source": [
        "# Transforming the dataframe df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhCCwfRVq15X"
      },
      "outputs": [],
      "source": [
        "# Schema of the transformed df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QzXkYFBq15X"
      },
      "outputs": [],
      "source": [
        "# Checking the elements of the transformed df - Top 20 rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lml3iqCnq15Y"
      },
      "outputs": [],
      "source": [
        "# Storing the transformed df in S3 bucket to prevent repetition of steps again\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwv7-1zSq15Y"
      },
      "source": [
        "#### Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax7G53Sxq15Y"
      },
      "outputs": [],
      "source": [
        "# Building the model with hyperparameter tuning\n",
        "# Create ParamGrid for Cross Validation\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9eqc2UAq15Y",
        "outputId": "a30ccdca-6e51-48dc-e64f-b7ffb7d6dd64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 27:>                                                         (0 + 4) / 7]23/02/20 13:44:49 WARN MemoryStore: Not enough space to cache rdd_109_0 in memory! (computed 562.9 MB so far)\n",
            "23/02/20 13:44:49 WARN BlockManager: Persisting block rdd_109_0 to disk instead.\n",
            "23/02/20 13:44:49 WARN MemoryStore: Not enough space to cache rdd_109_3 in memory! (computed 562.9 MB so far)\n",
            "23/02/20 13:44:49 WARN BlockManager: Persisting block rdd_109_3 to disk instead.\n",
            "23/02/20 13:44:49 WARN MemoryStore: Not enough space to cache rdd_109_2 in memory! (computed 562.9 MB so far)\n",
            "23/02/20 13:44:49 WARN BlockManager: Persisting block rdd_109_2 to disk instead.\n",
            "23/02/20 13:44:50 WARN MemoryStore: Not enough space to cache rdd_109_1 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:44:50 WARN BlockManager: Persisting block rdd_109_1 to disk instead.\n",
            "[Stage 27:>                                                         (0 + 4) / 7]23/02/20 13:45:24 WARN MemoryStore: Not enough space to cache rdd_109_2 in memory! (computed 1300.8 MB so far)\n",
            "23/02/20 13:45:33 WARN MemoryStore: Not enough space to cache rdd_109_0 in memory! (computed 1951.2 MB so far)\n",
            "23/02/20 13:45:33 WARN MemoryStore: Not enough space to cache rdd_109_1 in memory! (computed 1951.2 MB so far)\n",
            "[Stage 27:=================================>                        (4 + 3) / 7]23/02/20 13:46:24 WARN MemoryStore: Not enough space to cache rdd_109_4 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:46:24 WARN BlockManager: Persisting block rdd_109_4 to disk instead.\n",
            "23/02/20 13:46:24 WARN MemoryStore: Not enough space to cache rdd_109_5 in memory! (computed 375.3 MB so far)\n",
            "23/02/20 13:46:24 WARN BlockManager: Persisting block rdd_109_5 to disk instead.\n",
            "23/02/20 13:46:36 WARN MemoryStore: Not enough space to cache rdd_109_6 in memory! (computed 1300.8 MB so far)\n",
            "23/02/20 13:46:36 WARN BlockManager: Persisting block rdd_109_6 to disk instead.\n",
            "[Stage 27:=================================>                        (4 + 3) / 7]23/02/20 13:46:52 WARN MemoryStore: Not enough space to cache rdd_109_4 in memory! (computed 1951.2 MB so far)\n",
            "[Stage 27:=================================================>        (6 + 1) / 7]23/02/20 13:47:11 WARN MemoryStore: Not enough space to cache rdd_109_6 in memory! (computed 1951.2 MB so far)\n",
            "[Stage 29:========>                                                 (1 + 4) / 7]23/02/20 13:47:25 WARN MemoryStore: Not enough space to cache rdd_109_4 in memory! (computed 250.2 MB so far)\n",
            "23/02/20 13:47:25 WARN MemoryStore: Not enough space to cache rdd_109_0 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:47:25 WARN MemoryStore: Not enough space to cache rdd_109_2 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:47:26 WARN MemoryStore: Not enough space to cache rdd_109_1 in memory! (computed 844.3 MB so far)\n",
            "[Stage 29:=================================================>        (6 + 1) / 7]23/02/20 13:47:42 WARN MemoryStore: Not enough space to cache rdd_109_6 in memory! (computed 1951.2 MB so far)\n",
            "[Stage 31:========>                                                 (1 + 4) / 7]23/02/20 13:47:55 WARN MemoryStore: Not enough space to cache rdd_109_4 in memory! (computed 250.2 MB so far)\n",
            "23/02/20 13:47:55 WARN MemoryStore: Not enough space to cache rdd_109_1 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:47:55 WARN MemoryStore: Not enough space to cache rdd_109_0 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:47:55 WARN MemoryStore: Not enough space to cache rdd_109_2 in memory! (computed 844.3 MB so far)\n",
            "[Stage 31:=================================================>        (6 + 1) / 7]23/02/20 13:48:14 WARN MemoryStore: Not enough space to cache rdd_109_6 in memory! (computed 1951.2 MB so far)\n",
            "[Stage 33:========>                                                 (1 + 4) / 7]23/02/20 13:48:29 WARN MemoryStore: Not enough space to cache rdd_109_1 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:48:29 WARN MemoryStore: Not enough space to cache rdd_109_2 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:48:29 WARN MemoryStore: Not enough space to cache rdd_109_0 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:48:30 WARN MemoryStore: Not enough space to cache rdd_109_4 in memory! (computed 250.2 MB so far)\n",
            "[Stage 33:=================================================>        (6 + 1) / 7]23/02/20 13:48:47 WARN MemoryStore: Not enough space to cache rdd_109_6 in memory! (computed 1951.2 MB so far)\n",
            "[Stage 35:========>                                                 (1 + 4) / 7]23/02/20 13:49:01 WARN MemoryStore: Not enough space to cache rdd_109_2 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:49:01 WARN MemoryStore: Not enough space to cache rdd_109_0 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:49:01 WARN MemoryStore: Not enough space to cache rdd_109_1 in memory! (computed 844.3 MB so far)\n",
            "23/02/20 13:49:02 WARN MemoryStore: Not enough space to cache rdd_109_4 in memory! (computed 250.2 MB so far)\n",
            "[Stage 35:=================================================>        (6 + 1) / 7]23/02/20 13:49:19 WARN MemoryStore: Not enough space to cache rdd_109_6 in memory! (computed 1951.2 MB so far)\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_classifier = DecisionTreeClassifier(labelCol=\"target\").fit(training_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htkgp6GBq15Y"
      },
      "outputs": [],
      "source": [
        "df_predictions = df_classifier.transform(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3djZEWanq15Y",
        "outputId": "90b1b760-3271-4a88-e96c-c7a635c0d289"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 37:>                                                         (0 + 1) / 1]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+--------------------+--------------------+----------+\n",
            "|            features|target|       rawPrediction|         probability|prediction|\n",
            "+--------------------+------+--------------------+--------------------+----------+\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "|(123,[0,3,5,18,10...|     0|[1.9622281E7,8921...|[0.68743431027226...|       0.0|\n",
            "+--------------------+------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3UIg-cgq15Y"
      },
      "outputs": [],
      "source": [
        "# Run cross-validation steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcvzVcPFq15Y"
      },
      "outputs": [],
      "source": [
        "# Fitting the models on transformed df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLlXckaFq15Y"
      },
      "outputs": [],
      "source": [
        "# Best model from the results of cross-validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quNekpNXq15Y"
      },
      "source": [
        "#### Model Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrglTwQeq15Z"
      },
      "source": [
        "Required Steps:\n",
        "- Fit on test data\n",
        "- Performance analysis\n",
        "    - Appropriate Metric with reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvUdv7N5q15Z",
        "outputId": "3eb368e2-27ab-49a1-966d-91b22cf37980"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_accuracy = MulticlassClassificationEvaluator(labelCol=\"target\",metricName=\"accuracy\").evaluate(df_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vCU260yq15Z",
        "outputId": "a7bda883-ca1d-4b78-fd0c-2e21ccf29a60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.694056762265776"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRM2UfMwq15Z",
        "outputId": "2469da07-5e49-4986-e14f-4a0244a9316d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_precision = MulticlassClassificationEvaluator(labelCol=\"target\",metricName=\"weightedPrecision\").evaluate(df_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AggbdqUeq15Z",
        "outputId": "41e03699-7aa8-4329-a06a-3f4c413d39b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7441164064547349"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nk0RFdBq15Z",
        "outputId": "6b4fdc41-ede3-437d-fc69-2e8785d6b3bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Assuming you have a decision tree model called \"dt\" and a test dataset called \"testData\"\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"target\", metricName=\"weightedRecall\")\n",
        "recall = evaluator.evaluate(df_predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROrq580Mq15Z",
        "outputId": "e3e8ba12-2930-4562-b5d8-e0bb9190c07c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.694056762265776"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzoTp-fyq15Z"
      },
      "source": [
        "#### Summary of the best Decision Tree model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpL1INXPq15a",
        "outputId": "864ae588-35c3-4cd8-ff16-39b4e561d941"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41719)\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n"
          ]
        },
        {
          "ename": "Py4JNetworkError",
          "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:41719)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3661/685520271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create the Random Forest model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m rfparamGrid = (ParamGridBuilder()\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, numTrees, featureSubsetStrategy, seed, subsamplingRate)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         self._java_obj = self._new_java_obj(\n\u001b[0;32m-> 1110\u001b[0;31m             \"org.apache.spark.ml.classification.RandomForestClassifier\", self.uid)\n\u001b[0m\u001b[1;32m   1111\u001b[0m         self._setDefault(maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,\n\u001b[1;32m   1112\u001b[0m                          \u001b[0mmaxMemoryInMB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcacheNodeIds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointInterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[1;32m   1650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:41719)"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\")\n",
        "\n",
        "rfparamGrid = (ParamGridBuilder()\n",
        "\n",
        "               .addGrid(rf.maxDepth, [2, 5, 10])\n",
        "\n",
        "               .addGrid(rf.maxBins, [5, 10, 20])\n",
        "\n",
        "               .addGrid(rf.numTrees, [5, 20, 50])\n",
        "             .build())\n",
        "\n",
        "rfevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "rfcv = CrossValidator(estimator = rf,\n",
        "                      estimatorParamMaps = rfparamGrid,\n",
        "                      evaluator = rfevaluator,\n",
        "                      numFolds = 5)\n",
        "\n",
        "rfcvModel = rfcv.fit(training_df)\n",
        "print(rfcvModel)\n",
        "rfpredictions = rfcvModel.transform(testing_df)\n",
        "\n",
        "print('Accuracy:', rfevaluator.evaluate(rfpredictions))\n",
        "print('AUC:', BinaryClassificationMetrics(rfpredictions['label','prediction'].rdd).areaUnderROC)\n",
        "print('PR:', BinaryClassificationMetrics(rfpredictions['label','prediction'].rdd).areaUnderPR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc6eeb8Sq15a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_p7y8CVq15a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}