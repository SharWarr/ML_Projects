{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharWarr/ML_Projects/blob/main/Ecommerce_Churn_Project/Notebook_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b97006",
      "metadata": {
        "id": "f6b97006"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook --no-browser\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
        "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
        "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d5d113",
      "metadata": {
        "id": "30d5d113"
      },
      "outputs": [],
      "source": [
        "# Spark environment\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3af8912",
      "metadata": {
        "id": "b3af8912",
        "outputId": "750f5fb1-b163-46b7-fb7b-a0076e7dcbd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/02/20 04:48:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ip-172-31-92-239.ec2.internal:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>demo</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd96418a190>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAX_MEMORY = \"14G\"\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"demo\") \\\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baaafa5b",
      "metadata": {
        "id": "baaafa5b",
        "outputId": "ee23a1c6-eb29-4fdc-fa85-b2620bf04905"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df=spark.read.parquet(\"Cleaned_df_final_parquet.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2914ca9a",
      "metadata": {
        "id": "2914ca9a",
        "outputId": "c117fe6d-1b9e-43ea-bdd9-d8f7d806c731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- category_id: string (nullable = true)\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- user_session: string (nullable = true)\n",
            " |-- category_1: string (nullable = true)\n",
            " |-- category_2: string (nullable = true)\n",
            " |-- Hour: integer (nullable = true)\n",
            " |-- brand_new: string (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e0a663b",
      "metadata": {
        "id": "0e0a663b"
      },
      "outputs": [],
      "source": [
        "df = df.drop(\"user_id\",\"product_id\",\"category_id\",\"brand\",\"user_session\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c90702",
      "metadata": {
        "id": "34c90702",
        "outputId": "a2d5c9af-4ec4-41a9-8cec-a386a28be016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------+------+-----------+--------------------+----+---------+------+-----------+\n",
            "|event_type|       category_code| price| category_1|          category_2|Hour|brand_new|target|Hour_binned|\n",
            "+----------+--------------------+------+-----------+--------------------+----+---------+------+-----------+\n",
            "|      view|electronics.smart...|341.74|electronics|                null|  10|   xiaomi|     0|        1.0|\n",
            "|      view|         no category| 36.04|no category|                null|  10| no brand|     0|        1.0|\n",
            "|      view|         no category| 34.11|no category|                null|  10|   Others|     0|        1.0|\n",
            "|      view|         no category| 63.06|no category|                null|  13|   Others|     0|        2.0|\n",
            "|      view|         no category|341.91|no category|                null|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|362.34|no category|                null|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|341.91|no category|                null|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|392.38|no category|                null|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|339.28|no category|                null|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|448.84|no category|                null|  12| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|225.23| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...|  17|   Others|     0|        2.0|\n",
            "|      view|appliances.kitche...|225.23| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|228.47| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|electronics.smart...|952.03|electronics|                null|  13|   huawei|     0|        2.0|\n",
            "|      view|electronics.smart...|196.91|electronics|                null|   8|   xiaomi|     0|        1.0|\n",
            "|      view|electronics.smart...|153.98|electronics|                null|   5|   huawei|     0|        0.0|\n",
            "|      view|electronics.smart...|166.54|electronics|                null|   5|   huawei|     0|        0.0|\n",
            "+----------+--------------------+------+-----------+--------------------+----+---------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "bucketizer = Bucketizer(splits=[ 0, 6, 12, 18, 24 ],inputCol=\"Hour\", outputCol=\"Hour_binned\")\n",
        "df_buck = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
        "\n",
        "df_buck.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22bccd3",
      "metadata": {
        "id": "f22bccd3"
      },
      "outputs": [],
      "source": [
        "df_buck = df_buck.fillna(value ='no category',subset =['category_2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ede0221",
      "metadata": {
        "id": "2ede0221",
        "outputId": "4d56ad10-dffa-4b30-fe30-c74d1e736de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------+------+-----------+--------------------+----+---------+------+-----------+\n",
            "|event_type|       category_code| price| category_1|          category_2|Hour|brand_new|target|Hour_binned|\n",
            "+----------+--------------------+------+-----------+--------------------+----+---------+------+-----------+\n",
            "|      view|electronics.smart...|341.74|electronics|         no category|  10|   xiaomi|     0|        1.0|\n",
            "|      view|         no category| 36.04|no category|         no category|  10| no brand|     0|        1.0|\n",
            "|      view|         no category| 34.11|no category|         no category|  10|   Others|     0|        1.0|\n",
            "|      view|         no category| 63.06|no category|         no category|  13|   Others|     0|        2.0|\n",
            "|      view|         no category|341.91|no category|         no category|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|362.34|no category|         no category|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|341.91|no category|         no category|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|392.38|no category|         no category|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|339.28|no category|         no category|  12| no brand|     0|        2.0|\n",
            "|      view|         no category|448.84|no category|         no category|  12| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|225.23| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...|  17|   Others|     0|        2.0|\n",
            "|      view|appliances.kitche...|225.23| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|228.47| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...|  17| no brand|     0|        2.0|\n",
            "|      view|electronics.smart...|952.03|electronics|         no category|  13|   huawei|     0|        2.0|\n",
            "|      view|electronics.smart...|196.91|electronics|         no category|   8|   xiaomi|     0|        1.0|\n",
            "|      view|electronics.smart...|153.98|electronics|         no category|   5|   huawei|     0|        0.0|\n",
            "|      view|electronics.smart...|166.54|electronics|         no category|   5|   huawei|     0|        0.0|\n",
            "+----------+--------------------+------+-----------+--------------------+----+---------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_buck.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ff43f8",
      "metadata": {
        "id": "96ff43f8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import IntegerType,FloatType\n",
        "df_buck = df_buck.withColumn(\"Hour_binned\", df_buck[\"Hour_binned\"].cast(IntegerType()))\n",
        "df_buck = df_buck.withColumn(\"price\", df_buck[\"price\"].cast(FloatType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8690aef",
      "metadata": {
        "id": "d8690aef",
        "outputId": "c7653a85-8a67-44b1-907a-bf0c3d89727d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- category_1: string (nullable = true)\n",
            " |-- category_2: string (nullable = false)\n",
            " |-- Hour: integer (nullable = true)\n",
            " |-- brand_new: string (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            " |-- Hour_binned: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_buck.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8648df",
      "metadata": {
        "id": "dd8648df"
      },
      "outputs": [],
      "source": [
        "df_buck = df_buck.drop(\"Hour\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f638ab6d",
      "metadata": {
        "id": "f638ab6d"
      },
      "outputs": [],
      "source": [
        "# cast the click column to interger data type. \n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_buck = df_buck.withColumn(\"y\", df[\"target\"].cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed94035b",
      "metadata": {
        "id": "ed94035b"
      },
      "outputs": [],
      "source": [
        "#A user defined function for sampling the data. \n",
        "\n",
        "from math import floor\n",
        "from pyspark.sql.functions import rand\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def stratifiedSample(df, N, labelCol=\"y\"):\n",
        "    ctx = df.groupby(labelCol).count()\n",
        "    ctx = ctx.withColumn('frac', col(\"count\") / df.count())\n",
        "    frac = ctx.select(\"y\", \"frac\").rdd.collectAsMap()\n",
        "    pos = int(floor(frac[1] * N))\n",
        "    neg = int(floor(frac[0] * N))\n",
        "    posDF = df.filter(col(labelCol) == 1).orderBy(rand()).limit(pos)\n",
        "    negDF = df.filter(col(labelCol) == 0).orderBy(rand()).limit(neg)\n",
        "    return posDF.unionAll(negDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0652895",
      "metadata": {
        "id": "a0652895",
        "outputId": "545703e0-65b1-4130-df1b-b85df89eec5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# use the function created to sample the 5M rows from the complete dataset. \n",
        "\n",
        "xdf = stratifiedSample(df_buck, 5_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08dfc723",
      "metadata": {
        "scrolled": true,
        "id": "08dfc723",
        "outputId": "4d4b0c59-ff07-4ac4-bd79-2c3b4019272b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "|  y|  count|\n",
            "+---+-------+\n",
            "|  1|1668207|\n",
            "|  0|3331792|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xdf.groupby(\"y\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2beb970a",
      "metadata": {
        "id": "2beb970a",
        "outputId": "6a5cd321-41e8-4721-f069-24a1bc89f537"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+-------------------+\n",
            "|  y|  count|               frac|\n",
            "+---+-------+-------------------+\n",
            "|  1|1668207|0.33364146672829337|\n",
            "|  0|3331792| 0.6663585332717067|\n",
            "+---+-------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xdf.groupby(\"y\").count().withColumn(\"frac\", col(\"count\") / xdf.count()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c75509",
      "metadata": {
        "scrolled": true,
        "id": "08c75509",
        "outputId": "0a8aa178-e07c-4f9e-872a-80257257cfa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- category_1: string (nullable = true)\n",
            " |-- category_2: string (nullable = false)\n",
            " |-- brand_new: string (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            " |-- Hour_binned: integer (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xdf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df5cd3e",
      "metadata": {
        "id": "4df5cd3e",
        "outputId": "f37d8459-05d4-4d95-8702-b49c2aff2f3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "|  y|  count|\n",
            "+---+-------+\n",
            "|  1|1668207|\n",
            "|  0|3331792|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xdf.groupBy(\"y\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a3e3b4",
      "metadata": {
        "scrolled": false,
        "id": "53a3e3b4",
        "outputId": "6b79e3ae-c6c1-4d4c-f070-01a73e0ad6ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+\n",
            "|target|  count|\n",
            "+------+-------+\n",
            "|     1|1668207|\n",
            "|     0|3331792|\n",
            "+------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xdf.groupBy(\"target\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67cc46a",
      "metadata": {
        "id": "b67cc46a"
      },
      "outputs": [],
      "source": [
        "xdf = xdf.drop(\"target\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "487c3f62",
      "metadata": {
        "scrolled": true,
        "id": "487c3f62",
        "outputId": "cb5b2408-cbc7-41cd-9aaf-7453449be2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- category_1: string (nullable = true)\n",
            " |-- category_2: string (nullable = false)\n",
            " |-- brand_new: string (nullable = true)\n",
            " |-- Hour_binned: integer (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xdf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf1e1fd",
      "metadata": {
        "scrolled": false,
        "id": "dcf1e1fd",
        "outputId": "76cd42cc-65a7-49bf-c724-bab0a7566899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- category_1: string (nullable = true)\n",
            " |-- category_2: string (nullable = true)\n",
            " |-- brand_new: string (nullable = true)\n",
            " |-- Hour_binned: integer (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xdf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8581019",
      "metadata": {
        "scrolled": true,
        "id": "a8581019",
        "outputId": "17e81c6e-9e9a-41ff-f7b7-dd61dc4cb889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------+------+-----------+--------------------+---------+------+-----------+---+------------------+\n",
            "|event_type|       category_code| price| category_1|          category_2|brand_new|target|Hour_binned|  y|Hour_binned_posEnc|\n",
            "+----------+--------------------+------+-----------+--------------------+---------+------+-----------+---+------------------+\n",
            "|      view|electronics.smart...|341.74|electronics|         no category|   xiaomi|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view|         no category| 36.04|no category|         no category| no brand|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view|         no category| 34.11|no category|         no category|   Others|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view|         no category| 63.06|no category|         no category|   Others|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|         no category|341.91|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|         no category|362.34|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|         no category|341.91|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|         no category|392.38|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|         no category|339.28|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|         no category|448.84|no category|         no category| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|appliances.kitche...|225.23| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...|   Others|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|appliances.kitche...|225.23| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|appliances.kitche...|228.47| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|appliances.kitche...|283.12| appliances|kitchen.refrigera...| no brand|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|electronics.smart...|952.03|electronics|         no category|   huawei|     0|          2|  0|     (3,[2],[1.0])|\n",
            "|      view|electronics.smart...|196.91|electronics|         no category|   xiaomi|     0|          1|  0|     (3,[1],[1.0])|\n",
            "|      view|electronics.smart...|153.98|electronics|         no category|   huawei|     0|          0|  0|     (3,[0],[1.0])|\n",
            "|      view|electronics.smart...|166.54|electronics|         no category|   huawei|     0|          0|  0|     (3,[0],[1.0])|\n",
            "+----------+--------------------+------+-----------+--------------------+---------+------+-----------+---+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# onehot encoding \n",
        "#import the onehot encoder\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "#create the encoder object\n",
        "ohe = OneHotEncoderEstimator(inputCols=['Hour_binned'], outputCols=['Hour_binned_posEnc'])\n",
        "#fit the obejct to the dataframe\n",
        "oh_encoder = ohe.fit(df_buck)\n",
        "#tranform the dataframe, by adding the \n",
        "encoded = oh_encoder.transform(df_buck)\n",
        "\n",
        "encoded.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "597f966c",
      "metadata": {
        "id": "597f966c",
        "outputId": "802a98da-d1d0-4d53-96ab-505fa4f15628"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|event_type_ixEnc|\n",
            "+----------------+\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "|   (2,[0],[1.0])|\n",
            "+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import the string indexer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "#similar to the one hot encoder, create a string indexer object and fit it to the dataframe, use the fitted object to transform the dataset.\n",
        "si = StringIndexer(inputCol='event_type', outputCol='event_type_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['event_type_ix'], outputCols=['event_type_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"event_type_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd0527e",
      "metadata": {
        "scrolled": true,
        "id": "4fd0527e",
        "outputId": "fcebc5a7-a283-4f64-8f36-c7e9b492f582"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|category_1_ixEnc|\n",
            "+----------------+\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[1],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[2],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "|  (13,[0],[1.0])|\n",
            "+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#similar to the one hot encoder, create a string indexer object and fit it to the dataframe, use the fitted object to transform the dataset.\n",
        "si = StringIndexer(inputCol='category_1', outputCol='category_1_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['category_1_ix'], outputCols=['category_1_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"category_1_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c0e35c",
      "metadata": {
        "scrolled": false,
        "id": "20c0e35c",
        "outputId": "bea66369-3a8d-40e2-c73c-cc885cabc4d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|category_2_ixEnc|\n",
            "+----------------+\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[3],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "|  (85,[0],[1.0])|\n",
            "+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "si = StringIndexer(inputCol='category_2', outputCol='category_2_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['category_2_ix'], outputCols=['category_2_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"category_2_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0320fbf3",
      "metadata": {
        "id": "0320fbf3",
        "outputId": "9dda2b80-d2fa-4fce-fe04-5360fc4d2fef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|brand_new_ixEnc|\n",
            "+---------------+\n",
            "| (20,[4],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[0],[1.0])|\n",
            "| (20,[0],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[0],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[1],[1.0])|\n",
            "| (20,[5],[1.0])|\n",
            "| (20,[4],[1.0])|\n",
            "| (20,[5],[1.0])|\n",
            "| (20,[5],[1.0])|\n",
            "+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "si = StringIndexer(inputCol='brand_new', outputCol='brand_new_ix')\n",
        "encoded = si.fit(encoded).transform(encoded)\n",
        "#use the output of the sting indexer as an input to the onehot encoder. \n",
        "ohe = OneHotEncoderEstimator(inputCols=['brand_new_ix'], outputCols=['brand_new_ixEnc'])\n",
        "oh_encoder = ohe.fit(encoded)\n",
        "encoded = oh_encoder.transform(encoded)\n",
        "\n",
        "encoded.select(\"brand_new_ixEnc\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00a0c0dd",
      "metadata": {
        "id": "00a0c0dd",
        "outputId": "89aca4e1-d104-470f-f6aa-584f29330435"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['event_type',\n",
              " 'category_code',\n",
              " 'price',\n",
              " 'category_1',\n",
              " 'category_2',\n",
              " 'brand_new',\n",
              " 'target',\n",
              " 'Hour_binned',\n",
              " 'y',\n",
              " 'Hour_binned_posEnc',\n",
              " 'event_type_ix',\n",
              " 'event_type_ixEnc',\n",
              " 'category_1_ix',\n",
              " 'category_1_ixEnc',\n",
              " 'category_2_i',\n",
              " 'category_2_ix',\n",
              " 'category_2_ixEnc']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8245bbe7",
      "metadata": {
        "id": "8245bbe7",
        "outputId": "095091f0-d05f-4e9b-a048-6fbdb1814022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|(124,[1,3,5,18,10...|\n",
            "|(124,[1,3,6,18,10...|\n",
            "|(124,[1,3,6,18,10...|\n",
            "|(124,[2,3,6,18,10...|\n",
            "|(124,[2,3,6,18,10...|\n",
            "|(124,[2,3,6,18,10...|\n",
            "|(124,[2,3,6,18,10...|\n",
            "|(124,[2,3,6,18,10...|\n",
            "|(124,[2,3,6,18,10...|\n",
            "|(124,[2,3,6,18,10...|\n",
            "|(124,[2,3,7,21,10...|\n",
            "|(124,[2,3,7,21,10...|\n",
            "|(124,[2,3,7,21,10...|\n",
            "|(124,[2,3,7,21,10...|\n",
            "|(124,[2,3,7,21,10...|\n",
            "|(124,[2,3,7,21,10...|\n",
            "|(124,[2,3,5,18,10...|\n",
            "|(124,[1,3,5,18,10...|\n",
            "|(124,[0,3,5,18,10...|\n",
            "|(124,[0,3,5,18,10...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#import the vector assembler \n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#create the assembler object\n",
        "assembler = VectorAssembler(inputCols=['Hour_binned_posEnc',\n",
        " 'event_type_ixEnc',\n",
        " 'category_1_ixEnc',\n",
        " 'category_2_ixEnc',\n",
        " 'brand_new_ixEnc',\n",
        " 'price'], outputCol=\"features\")\n",
        "#transform the data frame using the assembler object. \n",
        "encoded = assembler.transform(encoded)\n",
        "encoded.select(\"features\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337d7c74",
      "metadata": {
        "id": "337d7c74",
        "outputId": "cd0982fb-77d1-408f-b463-815fa2cfdc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---+\n",
            "|            features|  y|\n",
            "+--------------------+---+\n",
            "|(124,[1,3,5,18,10...|  0|\n",
            "|(124,[1,3,6,18,10...|  0|\n",
            "|(124,[1,3,6,18,10...|  0|\n",
            "|(124,[2,3,6,18,10...|  0|\n",
            "|(124,[2,3,6,18,10...|  0|\n",
            "|(124,[2,3,6,18,10...|  0|\n",
            "|(124,[2,3,6,18,10...|  0|\n",
            "|(124,[2,3,6,18,10...|  0|\n",
            "|(124,[2,3,6,18,10...|  0|\n",
            "|(124,[2,3,6,18,10...|  0|\n",
            "|(124,[2,3,7,21,10...|  0|\n",
            "|(124,[2,3,7,21,10...|  0|\n",
            "|(124,[2,3,7,21,10...|  0|\n",
            "|(124,[2,3,7,21,10...|  0|\n",
            "|(124,[2,3,7,21,10...|  0|\n",
            "|(124,[2,3,7,21,10...|  0|\n",
            "|(124,[2,3,5,18,10...|  0|\n",
            "|(124,[1,3,5,18,10...|  0|\n",
            "|(124,[0,3,5,18,10...|  0|\n",
            "|(124,[0,3,5,18,10...|  0|\n",
            "+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "encoded.select(\"features\",\"y\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a54cf97e",
      "metadata": {
        "id": "a54cf97e"
      },
      "outputs": [],
      "source": [
        "model_df = encoded.select(\"features\",\"y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f6c081",
      "metadata": {
        "scrolled": true,
        "id": "70f6c081",
        "outputId": "f56f9350-2ff3-4aec-b773-6748f2716724"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 325:>                                                        (0 + 4) / 7]23/02/20 05:27:32 WARN MemoryStore: Not enough space to cache rdd_679_0 in memory! (computed 551.5 MB so far)\n",
            "23/02/20 05:27:32 WARN BlockManager: Persisting block rdd_679_0 to disk instead.\n",
            "23/02/20 05:27:32 WARN MemoryStore: Not enough space to cache rdd_679_2 in memory! (computed 551.5 MB so far)\n",
            "23/02/20 05:27:32 WARN BlockManager: Persisting block rdd_679_2 to disk instead.\n",
            "23/02/20 05:27:32 WARN MemoryStore: Not enough space to cache rdd_679_3 in memory! (computed 552.0 MB so far)\n",
            "23/02/20 05:27:32 WARN BlockManager: Persisting block rdd_679_3 to disk instead.\n",
            "[Stage 555:>                                                        (0 + 1) / 1]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+----------------------------------------+\n",
            "|y  |prediction|probability                             |\n",
            "+---+----------+----------------------------------------+\n",
            "|1  |1.0       |[0.24812254061668726,0.7518774593833127]|\n",
            "|1  |1.0       |[0.2481425740571454,0.7518574259428547] |\n",
            "|1  |1.0       |[0.24827348599466106,0.751726514005339] |\n",
            "|1  |1.0       |[0.24827348599466106,0.751726514005339] |\n",
            "|1  |1.0       |[0.2483496499459881,0.7516503500540119] |\n",
            "|1  |1.0       |[0.24855550439164803,0.7514444956083519]|\n",
            "|1  |1.0       |[0.24902244112909075,0.7509775588709092]|\n",
            "|1  |1.0       |[0.24914294974882775,0.7508570502511723]|\n",
            "|1  |1.0       |[0.24917777048457182,0.7508222295154282]|\n",
            "|1  |1.0       |[0.2491898246277536,0.7508101753722464] |\n",
            "+---+----------+----------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#split the data \n",
        "training_df,test_df=model_df.randomSplit([0.75,0.25])\n",
        "\n",
        "#import the logistic regression \n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "#Apply the logistic regression model\n",
        "log_reg=LogisticRegression(labelCol='y').fit(training_df)\n",
        "\n",
        "#Training Results\n",
        "train_results=log_reg.evaluate(training_df).predictions\n",
        "train_results.filter(train_results['y']==1).filter(train_results['prediction']==1).select(['y','prediction','probability']).show(10,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ece0a8",
      "metadata": {
        "scrolled": true,
        "id": "a5ece0a8",
        "outputId": "c0154f4a-7722-4278-a2d2-b9b8d6a09385"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 564:================================================>        (6 + 1) / 7]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy :  0.693945872751744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "accuracy = train_results.filter(train_results.y == train_results.prediction).count() / float(train_results.count())\n",
        "print(\"Accuracy : \",accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f81baa",
      "metadata": {
        "id": "94f81baa",
        "outputId": "325aee30-6827-4eb4-e73f-4ec5a072a103"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 598:=========>       (4 + 3) / 7][Stage 602:>                (0 + 1) / 7]23/02/20 06:13:12 WARN MemoryStore: Not enough space to cache rdd_1294_0 in memory! (computed 364.2 MB so far)\n",
            "[Stage 598:=========>       (4 + 3) / 7][Stage 604:>                (0 + 1) / 7]23/02/20 06:13:30 WARN MemoryStore: Not enough space to cache rdd_1294_0 in memory! (computed 364.2 MB so far)\n",
            "[Stage 614:=========>       (4 + 3) / 7][Stage 622:>                (0 + 1) / 7]23/02/20 06:16:39 WARN MemoryStore: Not enough space to cache rdd_1294_0 in memory! (computed 364.2 MB so far)\n",
            "[Stage 614:=========>       (4 + 3) / 7][Stage 624:>                (0 + 1) / 7]23/02/20 06:16:59 WARN MemoryStore: Not enough space to cache rdd_1294_0 in memory! (computed 364.2 MB so far)\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "tp = train_results.filter((col(\"y\") == 1) & (col(\"prediction\") == 1)).count()\n",
        "fp = train_results.filter((col(\"y\") == 0) & (col(\"prediction\") == 1)).count()\n",
        "fn = train_results.filter((col(\"y\") == 1) & (col(\"prediction\") == 0)).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19bec4bf",
      "metadata": {
        "id": "19bec4bf",
        "outputId": "bdd11607-7c33-4523-f804-757d781d9640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision:  0.8576116040223289\n",
            "Recall:  0.09938306315266973\n"
          ]
        }
      ],
      "source": [
        "# Calculate the precision and recall\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "\n",
        "# Print the precision and recall\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db79f5a9",
      "metadata": {
        "id": "db79f5a9"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379bc660",
      "metadata": {
        "id": "379bc660"
      },
      "outputs": [],
      "source": [
        "# Create vector assembler\n",
        "# Load data\n",
        "#pipeline\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['Hour_binned_posEnc',\n",
        " 'event_type_ixEnc',\n",
        " 'category_1_ixEnc',\n",
        " 'category_2_ixEnc',\n",
        " 'brand_new_ixEnc',\n",
        " 'price'], outputCol='features_pipeline')\n",
        "\n",
        "# Create logistic regression model\n",
        "lr = LogisticRegression(featuresCol='features_pipeline', labelCol=\"y\")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline(stages=[assembler, lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b94b9c",
      "metadata": {
        "id": "12b94b9c"
      },
      "outputs": [],
      "source": [
        "# Define grid of hyperparameters to test\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .addGrid(lr.maxIter, [10, 100]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e411b72",
      "metadata": {
        "id": "2e411b72"
      },
      "outputs": [],
      "source": [
        "# Define cross-validator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=param_grid,\n",
        "                    evaluator=BinaryClassificationEvaluator(),\n",
        "                    numFolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0418fe27",
      "metadata": {
        "id": "0418fe27",
        "outputId": "ae4dbcb6-bbe3-4a17-87a3-50728c972217"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/02/20 06:21:04 WARN CacheManager: Asked to cache already cached data.\n",
            "23/02/20 06:21:04 WARN CacheManager: Asked to cache already cached data.\n",
            "[Stage 661:>                (0 + 0) / 2][Stage 662:>                (0 + 4) / 7]23/02/20 06:21:18 WARN MemoryStore: Not enough space to cache rdd_1200_3 in memory! (computed 1486.3 MB so far)\n",
            "23/02/20 06:21:18 WARN MemoryStore: Not enough space to cache rdd_1200_2 in memory! (computed 1485.2 MB so far)\n",
            "23/02/20 06:21:28 WARN MemoryStore: Not enough space to cache rdd_1445_0 in memory! (computed 232.5 MB so far)\n",
            "23/02/20 06:21:28 WARN BlockManager: Persisting block rdd_1445_0 to disk instead.\n",
            "23/02/20 06:21:28 WARN MemoryStore: Not enough space to cache rdd_1445_2 in memory! (computed 226.2 MB so far)\n",
            "23/02/20 06:21:28 WARN BlockManager: Persisting block rdd_1445_2 to disk instead.\n",
            "23/02/20 06:21:29 WARN MemoryStore: Not enough space to cache rdd_1445_3 in memory! (computed 232.2 MB so far)\n",
            "23/02/20 06:21:29 WARN BlockManager: Persisting block rdd_1445_3 to disk instead.\n",
            "23/02/20 06:21:29 WARN MemoryStore: Not enough space to cache rdd_1445_1 in memory! (computed 233.4 MB so far)\n",
            "23/02/20 06:21:29 WARN BlockManager: Persisting block rdd_1445_1 to disk instead.\n",
            "[Stage 662:=========>       (4 + 3) / 7][Stage 664:>                (0 + 1) / 7]23/02/20 06:22:04 WARN MemoryStore: Not enough space to cache rdd_1200_4 in memory! (computed 1485.6 MB so far)\n",
            "23/02/20 06:22:07 WARN MemoryStore: Not enough space to cache rdd_1200_6 in memory! (computed 1482.0 MB so far)\n",
            "[Stage 722:================>                                        (2 + 4) / 7]"
          ]
        },
        {
          "ename": "IllegalArgumentException",
          "evalue": "'Field \"label\" does not exist.\\nAvailable fields: event_type, category_code, price, category_1, category_2, brand_new, target, Hour_binned, y, Hour_binned_posEnc, event_type_ix, event_type_ixEnc, category_1_ix, category_1_ixEnc, category_2_i, category_2_ix, category_2_ixEnc, brand_new_ix, brand_new_ixEnc, features, CrossValidator_b6f7896f094f_rand, features_pipeline, rawPrediction, probability, prediction'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o7409.evaluate.\n: java.lang.IllegalArgumentException: Field \"label\" does not exist.\nAvailable fields: event_type, category_code, price, category_1, category_2, brand_new, target, Hour_binned, y, Hour_binned_posEnc, event_type_ix, event_type_ixEnc, category_1_ix, category_1_ixEnc, category_2_i, category_2_ix, category_2_ixEnc, brand_new_ix, brand_new_ixEnc, features, CrossValidator_b6f7896f094f_rand, features_pipeline, rawPrediction, probability, prediction\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:74)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:77)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3640/1565617317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit cross-validator to training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcv_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use the best model to make predictions on the testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib64/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib64/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"label\" does not exist.\\nAvailable fields: event_type, category_code, price, category_1, category_2, brand_new, target, Hour_binned, y, Hour_binned_posEnc, event_type_ix, event_type_ixEnc, category_1_ix, category_1_ixEnc, category_2_i, category_2_ix, category_2_ixEnc, brand_new_ix, brand_new_ixEnc, features, CrossValidator_b6f7896f094f_rand, features_pipeline, rawPrediction, probability, prediction'"
          ]
        }
      ],
      "source": [
        "# Split data into training and testing sets\n",
        "training, testing = encoded.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Fit cross-validator to training data\n",
        "cv_model = cv.fit(training)\n",
        "\n",
        "# Use the best model to make predictions on the testing set\n",
        "predictions = cv_model.transform(testing)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "#area_under_curve = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c38a3fc9",
      "metadata": {
        "id": "c38a3fc9",
        "outputId": "a97f8931-ef26-4599-e8ab-6f13e30090ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 723:>                (0 + 0) / 2][Stage 724:>                (0 + 4) / 7]23/02/20 06:24:54 WARN MemoryStore: Not enough space to cache rdd_1200_2 in memory! (computed 1485.2 MB so far)\n",
            "23/02/20 06:25:02 WARN MemoryStore: Not enough space to cache rdd_1596_3 in memory! (computed 102.9 MB so far)\n",
            "23/02/20 06:25:02 WARN BlockManager: Persisting block rdd_1596_3 to disk instead.\n",
            "23/02/20 06:25:02 WARN MemoryStore: Not enough space to cache rdd_1596_0 in memory! (computed 154.8 MB so far)\n",
            "23/02/20 06:25:02 WARN BlockManager: Persisting block rdd_1596_0 to disk instead.\n",
            "23/02/20 06:25:02 WARN MemoryStore: Not enough space to cache rdd_1596_1 in memory! (computed 154.5 MB so far)\n",
            "23/02/20 06:25:02 WARN BlockManager: Persisting block rdd_1596_1 to disk instead.\n",
            "[Stage 724:==>              (1 + 4) / 7][Stage 726:>                (0 + 0) / 7]23/02/20 06:25:20 WARN MemoryStore: Not enough space to cache rdd_1200_4 in memory! (computed 1485.6 MB so far)\n",
            "[Stage 724:=========>       (4 + 3) / 7][Stage 726:>                (0 + 1) / 7]23/02/20 06:25:59 WARN MemoryStore: Not enough space to cache rdd_1200_6 in memory! (computed 2.2 GB so far)\n",
            "[Stage 837:>                (0 + 0) / 2][Stage 838:>                (0 + 4) / 7]23/02/20 06:30:17 WARN MemoryStore: Not enough space to cache rdd_1200_3 in memory! (computed 1486.3 MB so far)\n",
            "23/02/20 06:30:24 WARN MemoryStore: Not enough space to cache rdd_1851_1 in memory! (computed 154.5 MB so far)\n",
            "23/02/20 06:30:24 WARN BlockManager: Persisting block rdd_1851_1 to disk instead.\n",
            "23/02/20 06:30:24 WARN MemoryStore: Not enough space to cache rdd_1851_0 in memory! (computed 154.8 MB so far)\n",
            "23/02/20 06:30:24 WARN BlockManager: Persisting block rdd_1851_0 to disk instead.\n",
            "23/02/20 06:30:25 WARN MemoryStore: Not enough space to cache rdd_1851_2 in memory! (computed 150.8 MB so far)\n",
            "23/02/20 06:30:25 WARN BlockManager: Persisting block rdd_1851_2 to disk instead.\n",
            "23/02/20 06:30:25 WARN MemoryStore: Not enough space to cache rdd_1851_3 in memory! (computed 154.4 MB so far)\n",
            "23/02/20 06:30:25 WARN BlockManager: Persisting block rdd_1851_3 to disk instead.\n",
            "[Stage 838:=========>       (4 + 3) / 7][Stage 840:>                (0 + 1) / 7]23/02/20 06:31:05 WARN MemoryStore: Not enough space to cache rdd_1200_4 in memory! (computed 1485.6 MB so far)\n",
            "[Stage 901:>                (0 + 0) / 2][Stage 902:>                (0 + 4) / 7]23/02/20 06:33:59 WARN MemoryStore: Not enough space to cache rdd_1200_3 in memory! (computed 964.8 MB so far)\n",
            "23/02/20 06:34:10 WARN MemoryStore: Not enough space to cache rdd_2006_2 in memory! (computed 226.2 MB so far)\n",
            "23/02/20 06:34:10 WARN BlockManager: Persisting block rdd_2006_2 to disk instead.\n",
            "23/02/20 06:34:11 WARN MemoryStore: Not enough space to cache rdd_2006_1 in memory! (computed 233.4 MB so far)\n",
            "23/02/20 06:34:11 WARN BlockManager: Persisting block rdd_2006_1 to disk instead.\n",
            "[Stage 902:====>            (2 + 4) / 7][Stage 904:>                (0 + 0) / 7]23/02/20 06:34:39 WARN MemoryStore: Not enough space to cache rdd_1200_4 in memory! (computed 1485.6 MB so far)\n",
            "23/02/20 06:34:41 WARN MemoryStore: Not enough space to cache rdd_1200_5 in memory! (computed 1485.3 MB so far)\n",
            "[Stage 1016:===========>    (5 + 2) / 7][Stage 1018:>               (0 + 2) / 7]"
          ]
        }
      ],
      "source": [
        "area_under_curve = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530e6fdc",
      "metadata": {
        "id": "530e6fdc",
        "outputId": "7fb369f2-fdf5-4792-fbdb-f1d925d23a3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 1016:=============>  (6 + 1) / 7][Stage 1018:>               (0 + 3) / 7]"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'evaluator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3640/2231812280.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 1019:>               (0 + 0) / 2][Stage 1020:>               (0 + 4) / 7]23/02/20 06:39:11 WARN MemoryStore: Not enough space to cache rdd_1200_2 in memory! (computed 1485.2 MB so far)\n",
            "23/02/20 06:39:11 WARN MemoryStore: Not enough space to cache rdd_1200_3 in memory! (computed 1486.3 MB so far)\n",
            "23/02/20 06:39:20 WARN MemoryStore: Not enough space to cache rdd_2269_2 in memory! (computed 226.2 MB so far)\n",
            "23/02/20 06:39:20 WARN BlockManager: Persisting block rdd_2269_2 to disk instead.\n",
            "23/02/20 06:39:20 WARN MemoryStore: Not enough space to cache rdd_2269_0 in memory! (computed 232.5 MB so far)\n",
            "23/02/20 06:39:20 WARN BlockManager: Persisting block rdd_2269_0 to disk instead.\n",
            "23/02/20 06:39:21 WARN MemoryStore: Not enough space to cache rdd_2269_3 in memory! (computed 232.2 MB so far)\n",
            "23/02/20 06:39:21 WARN BlockManager: Persisting block rdd_2269_3 to disk instead.\n",
            "23/02/20 06:39:21 WARN MemoryStore: Not enough space to cache rdd_2269_1 in memory! (computed 233.4 MB so far)\n",
            "23/02/20 06:39:21 WARN BlockManager: Persisting block rdd_2269_1 to disk instead.\n",
            "[Stage 1020:=========>      (4 + 3) / 7][Stage 1022:>               (0 + 1) / 7]23/02/20 06:39:59 WARN MemoryStore: Not enough space to cache rdd_1200_5 in memory! (computed 1485.3 MB so far)\n",
            "23/02/20 06:40:01 WARN MemoryStore: Not enough space to cache rdd_1200_6 in memory! (computed 1482.0 MB so far)\n",
            "[Stage 1040:====>           (2 + 4) / 7][Stage 1042:>               (0 + 0) / 7]"
          ]
        }
      ],
      "source": [
        "print(evaluator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbe94e38",
      "metadata": {
        "id": "cbe94e38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}